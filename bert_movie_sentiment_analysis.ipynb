{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sudden-employee",
   "metadata": {
    "id": "UJ4J-KkH1S_P"
   },
   "source": [
    "# Sentiment Polarity Prediction Model Analysis Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-herald",
   "metadata": {
    "id": "UJ4J-KkH1S_P"
   },
   "source": [
    "This notebook analysis various models when used to carry out sentiment analysis on movie reviews. The goal of these sentiment analysis models is to classify a movie review as either positive or negative through document-level sentiment analysis.\n",
    "\n",
    "The models analysed are:\n",
    "\n",
    "1. A baseline Multinomial Naive Bayes model\n",
    "2. A Logistic Regression model\n",
    "3. A fine-tuned BERT model\n",
    "\n",
    "Code for a Multinomial Naive Bayes classification model was provided as a baseline model for the previous assignment. As part of that assignment, I took the initial code for this model, refactored it, expanded it and implemented a more stream lined version of this modle which I use in this notebook.\n",
    "\n",
    "The Logistic Regression model also stems from the previous assignment where as part of that assignment, I was to experiment with different models and parameters to improve on the accuracy of the initial baseline model. This Logistic Regression model was the best model that I experimented with and as a result deserves to be part of this analysis as it is the benchmark model in terms of accuracy.\n",
    "\n",
    "The new model brought to this analysis is the BERT model. Code for A basic implementation of this model was provided as part of this assignment. This implementation uses the Hugging Face [Transformers](https://huggingface.co/transformers/) library with [PyTorch](https://pytorch.org/) and [Lightning](https://www.pytorchlightning.ai/).\n",
    "\n",
    "---\n",
    "\n",
    "All of the models in this notebook are trained on the movie review polarity data of Pang and Lee 2004 [A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts](https://www.aclweb.org/anthology/P04-1035/). The dataset used in this paper is available at http://www.cs.cornell.edu/People/pabo/movie-review-data (section \"Sentiment polarity datasets\") and contains 1000 positive and 1000 negative reviews, with each review being tokenised, sentence-split (one sentence per line) and lowercased.\n",
    "\n",
    "In this dataset, each review has been assigned to 1 of 10 cross-validation folds by the authors. In order to compare the results of the different models outlined above, the models are evaluated and compared using an average of the 10-fold cross-validation accuracy scores.\n",
    "\n",
    "During this process, no special treatment is given to rare or unknown words. Unknown words in the test data are skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-balance",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-rubber",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-examination",
   "metadata": {},
   "source": [
    "### Packges needed to install\n",
    "\n",
    "In order to be able to re-implement this work, there are a number of packages you will need to install. The commands for these are as follows:\n",
    "\n",
    "- conda install numpy\n",
    "- conda install pandas\n",
    "- conda install tabulate\n",
    "- conda install matplotlib\n",
    "- conda install selenium\n",
    "- conda install tqdm\n",
    "- conda install scikit-learn (version > 0.24.0)\n",
    "- pip install torch==1.7.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "- pip install torchvision==0.8.2+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "- pip install torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "- pip install transformers\n",
    "- pip install pytorch-lightning\n",
    "- pip install pytorch-nlp\n",
    "- pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the torch version below following instructions on https://pytorch.org/get-started/locally/\n",
    "\n",
    "import sys\n",
    "\n",
    "# for why we use {sys.executable} see\n",
    "# https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install transformers\n",
    "\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install pytorch-lightning\n",
    "\n",
    "try:\n",
    "    import torchnlp\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install pytorch-nlp\n",
    "\n",
    "try:\n",
    "    import tensorboard\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install tensorboard\n",
    "\n",
    "!{sys.executable} -m pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-heater",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "equipped-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general packages\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "actual-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages for defining the tokeniser\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "younger-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages for clearing CUDA cache\n",
    "import gc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-testing",
   "metadata": {},
   "source": [
    "# packages for BERT tokanisation\n",
    "import torch\n",
    "import transformers\n",
    "import pytorch_lightning as pl\n",
    "import torchnlp\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-neighbor",
   "metadata": {},
   "source": [
    "### Import the classes and functions from the '.py' files\n",
    "\n",
    "Instead of having this notebook full of different functions with different applications, I decided to extract these from the notebook and to put them into a '.py' file. I could then read these functions in from the '.py' file without any problem which would enable me to keep this notebook compact and streamlined.\n",
    "\n",
    "For even more clarity, I have also broken the functions and classes down into three '.py' files. These files correspond to the functions and classes needed to:\n",
    "\n",
    "1. Load the data\n",
    "2. Train the Multinomial Naive Bayes model & the Logistic Regression model\n",
    "3. Train the BERT model\n",
    "\n",
    "The functions themselves are easily accessible and readable from the '.py' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "quarterly-diving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unlimited-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for loading the data\n",
    "%aimport Load_data_functions\n",
    "import Load_data_functions as ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mechanical-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for running the Naive Bayes & Logistic Regression models\n",
    "#%aimport Simple_model_functions\n",
    "#import Simple_model_functions as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "veterinary-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for running the BERT model\n",
    "%aimport Bert_functions\n",
    "import Bert_functions as bf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-quantity",
   "metadata": {},
   "source": [
    "### Increase the maximum number of rows displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "heated-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-smooth",
   "metadata": {},
   "source": [
    "### Define the location of the chromedriver\n",
    "\n",
    "In this code, I make use of a chromedriver to download the data from the internet. This is only done if the data is not already in your local folder. If you have cloned my original repo then the data will already be in the right place and you won't have to worry about the chromedriver. This data should be in the 'Data' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eastern-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver_location = 'chromedriver.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-combat",
   "metadata": {},
   "source": [
    "### Define the location of the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fantastic-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-embassy",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-sight",
   "metadata": {},
   "source": [
    "# Load in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-fitting",
   "metadata": {},
   "source": [
    "I implemented a nice way to load in the data in the last assignment and it makes sense to use these same functions and the same logic again here in this assignment. The logic behind the below *load_data* function is:\n",
    "\n",
    "1. Check if you have the data already downloaded and in the right place\n",
    "    1. If you don't have the data already downloaded, the data will be downloaded from the web and put into the specified data folder\n",
    "    2. If you have the data already downloaded then continue to step 2\n",
    "2. Read in all the files in the data\n",
    "3. Turn this data into a map of cross validation folds and class labels to their associated documents\n",
    "4. Return this map\n",
    "\n",
    "---\n",
    "\n",
    "The resulting output is a dictionary in the following format:\n",
    "\n",
    "    {(cross validation fold 1, 'pos'): [[list of sentences in doc1], [list of sentences in doc2], ...],\n",
    "     (cross validation fold 1, 'neg'): [[list of sentences in doc1], [list of sentences in doc2], ...],\n",
    "     .................................................................................................\n",
    "     (cross validation fold 10, 'pos'): [[list of sentences in doc1], [list of sentences in doc2], ...],\n",
    "     (cross validation fold 10, 'neg'): [[list of sentences in doc1], [list of sentences in doc2], ...],\n",
    "    }\n",
    "\n",
    "This dictionary has two enteries for each cross validation fold, one for the positive documents and one for the negative documents. Each value in the dictionary then contains a list of documents associated with that cross validation fold and class label pair. Each document in this list is made up of a list of sentences with each sentence, in turn, being made up of a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "RnQ1iJVT1S_b",
   "metadata": {
    "id": "RnQ1iJVT1S_b"
   },
   "outputs": [],
   "source": [
    "data_dict = ld.load_data(data_directory, chromedriver_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-giving",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-literature",
   "metadata": {
    "id": "Bko4aOjK1S_c"
   },
   "source": [
    "# Define the BERT Tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bko4aOjK1S_c",
   "metadata": {
    "id": "Bko4aOjK1S_c"
   },
   "source": [
    "We must set up a tokeniser for the BERT model. This ensures that the input sentence has been broken up into a sequence of tokens and that these tokens are changed into a numeric representation. When training the model, BERT uses these token sequence numeric represntations to predict the class of a given document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-conclusion",
   "metadata": {},
   "source": [
    "### Choose which size model to use\n",
    "\n",
    "There are three model types, which are based on size, the first being tiny, the second being a base model, and the third being a large model. Here, we can select which size we want to proceed with in this modelling.\n",
    "\n",
    "I decided to use the distilbert model as it was the most reliable and efficient size model to use. The base model and the large model ran out of GPU RAM under a few parameter configurations so for me the logical choice was to use the distilbert model. Despite its smaller size, its accuracy score held up well in comparison to what I saw using the larger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "choice-pakistan",
   "metadata": {
    "id": "fLx0sLs11S_d"
   },
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased' # tiny\n",
    "#model_name = 'bert-base-uncased'       # base\n",
    "#model_name = 'bert-large-uncased'      # large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-coating",
   "metadata": {},
   "source": [
    "### Define the tokeniser\n",
    "\n",
    "When it comes to the tokeniser used with this model, hugging face provides functionality where you can use the same tokeniser for whatever type of model you use. As a result, we can select any size model above without having to download a new tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "synthetic-symbol",
   "metadata": {
    "id": "fLx0sLs11S_d"
   },
   "outputs": [],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-conclusion",
   "metadata": {},
   "source": [
    "### Decide whether to pre-tokenise the data by splitting on whitespace\n",
    "\n",
    "We also have the option of selecting to use a pre-tokeniser. This could be used to split up the input sentences into whitespace delimited tokens before they are passed into the tokeniser. Populat BERT libraries can provide tools to help with this.\n",
    "\n",
    "In the case of our data, the P&L 04 corpus is already tokenised into words and punctuation so this step acrually doesn't matter as there are no whitespace characters in the data. As a result, this parameter will be set to **False** here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fLx0sLs11S_d",
   "metadata": {
    "id": "fLx0sLs11S_d"
   },
   "outputs": [],
   "source": [
    "force_whitespace_pre_tokeniser = False\n",
    "\n",
    "if force_whitespace_pre_tokeniser:\n",
    "    tokeniser.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-rubber",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-lesbian",
   "metadata": {},
   "source": [
    "# Test the Tokeniser on Example Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-matter",
   "metadata": {},
   "source": [
    "In this section, I will show the steps that this tokeniser does on an example input. You will see that the tokenisation process involves:\n",
    "\n",
    "1. Splitting the sentence into tokens\n",
    "2. Adding a start and end token to the sentence to specify these positions - [CLS] & [SEP] respectively\n",
    "3. Mapping each token in the sentence to a unique numeric encoding to be used in the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bibliographic-pricing",
   "metadata": {
    "id": "rL-dSUZS1S_e",
    "outputId": "6509cebd-c22e-4d21-af9a-b895c840d15f"
   },
   "outputs": [],
   "source": [
    "# Define a pre-tokenised input document\n",
    "example_batch = [['hello', 'world', '!'],\n",
    "                 [\"tokenisation\", \"'s\", \"fun\"],\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "professional-atmosphere",
   "metadata": {
    "id": "rL-dSUZS1S_e",
    "outputId": "6509cebd-c22e-4d21-af9a-b895c840d15f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |   sentence_num |   input_ids | tokens   |   word_ids |\n",
      "|---:|---------------:|------------:|:---------|-----------:|\n",
      "|  0 |              0 |         101 | [CLS]    |        nan |\n",
      "|  1 |              0 |        7592 | hello    |          0 |\n",
      "|  2 |              0 |        2088 | world    |          1 |\n",
      "|  3 |              0 |         999 | !        |          2 |\n",
      "|  4 |              0 |         102 | [SEP]    |        nan |\n",
      "\n",
      "\n",
      "|    |   sentence_num |   input_ids | tokens    |   word_ids |\n",
      "|---:|---------------:|------------:|:----------|-----------:|\n",
      "|  0 |              1 |         101 | [CLS]     |        nan |\n",
      "|  1 |              1 |       19204 | token     |          0 |\n",
      "|  2 |              1 |        6648 | ##isation |          0 |\n",
      "|  3 |              1 |        1005 | '         |          1 |\n",
      "|  4 |              1 |        1055 | s         |          1 |\n",
      "|  5 |              1 |        4569 | fun       |          2 |\n",
      "|  6 |              1 |         102 | [SEP]     |        nan |\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bf.print_token_summary_of_pretokenised_sentences(tokeniser, example_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rL-dSUZS1S_e",
   "metadata": {
    "id": "rL-dSUZS1S_e",
    "outputId": "6509cebd-c22e-4d21-af9a-b895c840d15f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | token   | encoding   |\n",
      "|---:|:--------|:-----------|\n",
      "|  0 | hello   | [7592]     |\n",
      "|  1 | world   | [2088]     |\n",
      "|  2 | !       | [999]      |\n",
      "\n",
      "\n",
      "|    | token        | encoding      |\n",
      "|---:|:-------------|:--------------|\n",
      "|  0 | tokenisation | [19204, 6648] |\n",
      "|  1 | 's           | [1005, 1055]  |\n",
      "|  2 | fun          | [4569]        |\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bf.print_encoding_of_pretokenised_sentences(tokeniser, example_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-skill",
   "metadata": {},
   "source": [
    "You will notice from the above outputs that in some cases, the tokeniser tokenises words even further than was done in the corpus loaded into this notebook. This is particularly evident in the outputs for the second example sentence where *'tokenisation'* gets turned into *'token'* and *'##isation'*. As a result of this, the token *'tokenisation'* gets mapped to two numeric encodings, one for each of its sub tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-dimension",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-matthew",
   "metadata": {
    "id": "cBHUA8DI1S_f"
   },
   "source": [
    "# Analyse the Distribution of Sequence Length Across the Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-cemetery",
   "metadata": {
    "id": "cBHUA8DI1S_f"
   },
   "source": [
    "The BERT models that have been made available to the public have only been trained up to a length of 512 subword units as memory requirements increase quadratically with the sequence length.\n",
    "\n",
    "In this section, I set out to anayse the number of tokens in each of the documents to get a feel for how the documents are distributed based on their length. This will enable me to see how much information would be lost in the case of the 512 subword unit restriction and how to proceed with implementing the BERT model based on this information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cBHUA8DI1S_f",
   "metadata": {
    "id": "cBHUA8DI1S_f"
   },
   "source": [
    "### Set the bin width of this distrbution\n",
    "\n",
    "The below bin width parameter specifies the width of the document length bins to put the documents into. In our case, we will specify **256** as this parameter value as this is half of the maximum sequence length of the model and will enable us to get a better feel for the document length distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "altered-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_width = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-router",
   "metadata": {},
   "source": [
    "### Create a table of the documents distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "neutral-violence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (926 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "distribution, max_length_bin = bf.get_distribution_of_document_lengths(data_dict, tokeniser, bin_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "whole-gender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | Bin_length   |   pos |   neg |   total |\n",
      "|---:|:-------------|------:|------:|--------:|\n",
      "|  0 | 0 -> 255     |     7 |    16 |      23 |\n",
      "|  1 | 256 -> 511   |   133 |   152 |     285 |\n",
      "|  2 | 512 -> 767   |   284 |   339 |     623 |\n",
      "|  3 | 768 -> 1023  |   277 |   288 |     565 |\n",
      "|  4 | 1024 -> 1279 |   155 |   125 |     280 |\n",
      "|  5 | 1280 -> 1535 |    78 |    46 |     124 |\n",
      "|  6 | 1536 -> 1791 |    32 |    19 |      51 |\n",
      "|  7 | 1792 -> 2047 |    18 |     8 |      26 |\n"
     ]
    }
   ],
   "source": [
    "bf.print_doc_breakdown_of_bins(distribution, bin_width, max_length_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kE6YL-5j1S_g",
   "metadata": {
    "id": "kE6YL-5j1S_g"
   },
   "source": [
    "As you can see from the above distribution table, only **15.5%** of the documents have document lengths less than 512 tokens. As a result of this alongside the constraints of the model, we will not be able to consider the document as a whole and instead will have to use a subset of the document. This subset, which will have a maximum sequence length of *(512)*, limits the amount of information that is available to us from each document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-liabilities",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5HvMC7GY1S_l",
   "metadata": {
    "id": "5HvMC7GY1S_l"
   },
   "source": [
    "# Create Training-Test Splits for Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-defeat",
   "metadata": {},
   "source": [
    "This is where we split the loaded dataset into train and test splits. In this dataset, there are 1000 positive documents and 1000 negative documents and we must split these 2000 documents into 10 different cross validation splits. This gives an 1800-200 train-test split for each of the 10 cross validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "BuCRUzNu1S_l",
   "metadata": {
    "id": "BuCRUzNu1S_l",
    "outputId": "10d706f0-b998-4dae-d9ed-73b397ce0984"
   },
   "outputs": [],
   "source": [
    "train_test_splits = ld.get_train_test_splits(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-macedonia",
   "metadata": {},
   "source": [
    "### Show the splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-crowd",
   "metadata": {},
   "source": [
    "We can visualise the number of documents in each cross validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld.count_docs_in_train_test_split(org_train_test_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-keeping",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0rCG_581S_l",
   "metadata": {
    "id": "f0rCG_581S_l"
   },
   "source": [
    "# Test the Slicing of Documents to the specified BERT Sequence Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-sponsorship",
   "metadata": {},
   "source": [
    "Here we will test out how the documents will be distributed when we subset them to the maximum specified sequence length on one of the cross validation fold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-basketball",
   "metadata": {},
   "source": [
    "### Set the maximum sequence length to put into our BERT model\n",
    "\n",
    "This parameter specifies the maximum sequence length that the documents will be subset to before they are fed into the BERT model. While this maximum specified length must be less than 512, as the BERT model cannot deal with any sequence with a greater length, its value can be changed to whatever value less than 512 that you choose.\n",
    "\n",
    "This parameters value is usually set to a number with base 2. Each increment in this value comes with a significant increase in the resource requirments to run the code and as a result, its value may need to be restricted to reflect the GPU you have available to you. The fine-tuning of BERT is where the process becomes the most memory intensive.\n",
    "\n",
    "As I have Google Collab Pro, I figured that I would be able to run my BERT model using the full sequence length of 512, however, when I ran the code using this maximum sequence length, I ran into the following error:\n",
    "\n",
    "    RuntimeError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 15.90 GiB total capacity; 14.79 GiB already allocated; 215.75 MiB free; 14.81 GiB reserved in total by PyTorch)\n",
    "\n",
    "This told me that the GPU I had access to at that time did not have enough memory to use all 512 tokens. For this reasons, I have chosen to restrict the model to a maximum sequence length of **256**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "unlikely-summary",
   "metadata": {
    "id": "kiXQsIX21S_f"
   },
   "outputs": [],
   "source": [
    "max_sequence_length = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-honey",
   "metadata": {},
   "source": [
    "### Select which cross validation fold to use in this test\n",
    "\n",
    "It makes sense to just use the first cross validation fold in this test, however, this parameter may be experimented with to see the results across the other folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "racial-birthday",
   "metadata": {
    "id": "41g-TtmV1S_m",
    "outputId": "4e0f228f-2b56-4174-8221-bba2fbd67362"
   },
   "outputs": [],
   "source": [
    "cv_num = 0\n",
    "cv_0_training_data = train_test_splits[cv_num][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-sleeping",
   "metadata": {},
   "source": [
    "### Specify how the sequence will be distributed across the start & end of the documents\n",
    "\n",
    "If we specify 512 tokens as our maximum specified length, there are a number of ways this subset of 512 tokens can be selected from the documents, these are:\n",
    "1. Using the first 512 tokens in the document\n",
    "2. Using the last 512 tokens in the document\n",
    "3. Combining a mix of the start tokens and the end tokens to form one list of tokens of length 512\n",
    "\n",
    "We can actually specify how we want these tokens to be selected from the documents using the below *start_end_fraction* parameter. This parameter specifies what proportion of the tokens in the final sequence to take from the start, with the rest of the tokens being taken from the end of the sequence.\n",
    "\n",
    "Eg. if:\n",
    "\n",
    "    start_end_fraction = 1   -->  we take all tokens from the start of the document\n",
    "    start_end_fraction = 0.5 -->  the tokens selected are split evenly between the start and the end\n",
    "    start_end_fraction = 0   -->  we take all tokens from the end of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abstract-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_end_fraction = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-aviation",
   "metadata": {},
   "source": [
    "### Create a summary table and a distribution table of the documents after they are sliced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41g-TtmV1S_m",
   "metadata": {
    "id": "41g-TtmV1S_m",
    "outputId": "4e0f228f-2b56-4174-8221-bba2fbd67362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1800 training documents in this cross validation fold\n"
     ]
    }
   ],
   "source": [
    "sliced_doc_summary_df, length_to_count_map = bf.test_document_slicer_on_train_test_split(cv_0_training_data, tokeniser, start_end_fraction, max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-synthesis",
   "metadata": {},
   "source": [
    "#### Output the sliced document summary\n",
    "\n",
    "This shows how the tokens in the first 10 documents were distributed according to the above *start_end_fraction* parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "foreign-branch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_idx</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>num_tokens_from_start</th>\n",
       "      <th>num_tokens_from_end</th>\n",
       "      <th>total_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>59</td>\n",
       "      <td>175</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>59</td>\n",
       "      <td>159</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>50</td>\n",
       "      <td>167</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>53</td>\n",
       "      <td>178</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>46</td>\n",
       "      <td>177</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>50</td>\n",
       "      <td>150</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>256</td>\n",
       "      <td>61</td>\n",
       "      <td>148</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>256</td>\n",
       "      <td>58</td>\n",
       "      <td>171</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>256</td>\n",
       "      <td>51</td>\n",
       "      <td>174</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>256</td>\n",
       "      <td>48</td>\n",
       "      <td>146</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_idx seq_len num_tokens_from_start num_tokens_from_end total_tokens\n",
       "0       0     256                    59                 175          234\n",
       "1       1     256                    59                 159          218\n",
       "2       2     256                    50                 167          217\n",
       "3       3     256                    53                 178          231\n",
       "4       4     256                    46                 177          223\n",
       "5       5     256                    50                 150          200\n",
       "6       6     256                    61                 148          209\n",
       "7       7     256                    58                 171          229\n",
       "8       8     256                    51                 174          225\n",
       "9       9     256                    48                 146          194"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliced_doc_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-screening",
   "metadata": {},
   "source": [
    "#### Output the distribution of each document sequence length\n",
    "\n",
    "This shows the number of documents with each document length once this document slicing has been completed.\n",
    "We should see the majority of documents having a length qual too the specified *'max_sequence_length'* parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "expensive-replacement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of each sequence length (document length --> # docs):\n",
      "53 --> 1\n",
      "243 --> 1\n",
      "254 --> 5\n",
      "255 --> 35\n",
      "256 --> 1758\n"
     ]
    }
   ],
   "source": [
    "print('Frequency of each sequence length (document length --> # docs):')\n",
    "for length in sorted(list(length_to_count_map.keys())):\n",
    "    print(length, \"-->\", length_to_count_map[length])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-release",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-karaoke",
   "metadata": {},
   "source": [
    "# Train a BERT classifier on each CV fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-olive",
   "metadata": {},
   "source": [
    "Now that I have tested how the document sequences are selected, it is time to see how the BERT model performs when trained on each of the cross validation folds.\n",
    "As this is how the Multinomial Naive Bayes model and the Logistic Regression model were evaluated, an independent BERT model will be trained on each of the cross validation folds to produce a model accuracy score. The accuracy scores from each of these models will then be averaged to obtain an overall accuracy score for the BERT model. We can then accurately compare these accuracy scores across the different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-packaging",
   "metadata": {},
   "source": [
    "### Set the hyperparameters used in the BERT classifier\n",
    "\n",
    "There are a number of hyper parameters that we need to set when running this model. These parameters are used to configure the model to a way that best suits the data we are using and our application. Some of these parameters define important things like the model learning rate and batch size.\n",
    "\n",
    "These parameters must be set carfully as they directly effect the models performance and the computational requirments needed to run these models. In the case of the batch size, inceses in this paramter result in a linear increase in the memory requirments of the model.\n",
    "\n",
    "While ideally we would like a batch size of 16 or 32 for efficient training, I have decided to select a batch size of 10 as this only requirs 12GB of GPU RAM. This is important as the GPU available only has 15GB RAM available.\n",
    "An increase to batch_size = 16, gives:\n",
    "\n",
    "    RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 15.90 GiB total capacity; 14.62 GiB already allocated; 87.75 MiB free; 14.94 GiB reserved in total by PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "celtic-blend",
   "metadata": {
    "id": "Fk1pXMdb1S_p",
    "outputId": "29034c5d-ba4e-4efc-eb8c-9e30b53c2b7a"
   },
   "outputs": [],
   "source": [
    "classifier_hyperparams = {# Encoder specific learning rate\n",
    "                          \"encoder_learning_rate\": 1e-05,\n",
    "\n",
    "                          # Classification head learning rate\n",
    "                          \"learning_rate\": 3e-05,\n",
    "\n",
    "                          # Number of epochs we want to keep the encoder model frozen\n",
    "                          \"nr_frozen_epochs\": 3,\n",
    "\n",
    "                          # How many subprocesses to use for data loading - 0 means data is loaded in the main process\n",
    "                          \"loader_workers\": 4,\n",
    "\n",
    "                          # Number of GPUs you have available\n",
    "                          \"gpus\": 1,\n",
    "\n",
    "                          # Number of documents used in each iteration of the model\n",
    "                          \"batch_size\": 10,\n",
    "                         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-stations",
   "metadata": {},
   "source": [
    "### Set the other parameters needed to train the model\n",
    "\n",
    "There are other parameters we can specify when training the model. These parameters are more general than the above hyperparameters.\n",
    "\n",
    "For instance, the *max_epochs* and the *model_patience* revolve around how many iterations the model does over the data. This value correlates directly to how long the model takes to train as the higher these numbers are, the more iterations needed and hence the more time the model will take.\n",
    "In this model, I have chosen to set the maximum number of epochs to **10**. This enables me to keep the time it takes to train the model to a minimum while also giving a reasonable amount of iterations for the BERT model to still be accurate.\n",
    "\n",
    "The *start_end_fraction* parameter is also specified here again. This parameter details how the tokens are extracted from the documents. This can have big implications on the model as we want to select a split where we maintain the majority of the sentiment details.\n",
    "Here, I have set this parameter to favour the end of the document. The reason for this was that I felt the majority of the documents sentiment would come at the end of the review as the writer summed up their feelings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "harmful-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {# Number of iterations the model does over the dataset\n",
    "                \"max_epochs\": 10,\n",
    "\n",
    "                # proportion of the tokens in the final sequence to take from the start of the document\n",
    "                \"start_end_fraction\": 0.0,  # set to 0.0001 to duplicate short documents\n",
    "\n",
    "                # The Pre-Processing Batch size\n",
    "                \"preproc_batch_size\": 8,\n",
    "\n",
    "                # Maximum number of epochs to allow the model to have without accuracy improvement\n",
    "                \"model_patience\": 5,\n",
    "\n",
    "                # Minimum change in accuracy the model should have\n",
    "                \"min_early_stopping_delta\": 0.0,\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-exclusive",
   "metadata": {},
   "source": [
    "### Put the predefined variables in a dictionary to be used when training the classifiers\n",
    "\n",
    "These are parameters that I have defined in the above code for various reasons that are also needed to define the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "predefined_variables = {\"model_name\": model_name,\n",
    "                        \"tokeniser\": tokeniser,\n",
    "                        \"max_sequence_length\": max_sequence_length,\n",
    "                       }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-charge",
   "metadata": {},
   "source": [
    "### Define a BERT classifier and a trainer for each CV fold using the above parameters\n",
    "\n",
    "I have stored all of the above defined parameters in a dictionaroes as it enables me to pass a only a few parameters to my defined function when defining the classification models. These values can then be accessed and unpacked inside this function.\n",
    "\n",
    "This function iterates through each cross-validation fold and creates a classifier and a model trainer for each of these. It also creates a model callback for each cross-validation which stores the values for the best model trained across all the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dependent-munich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining the classifiers for each CV fold:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb39a25f72814452be21c89e4cd5697e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining the trainers for each CV fold:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70876eeaccc4f828e5365cdb040ea89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "You requested GPUs: [0]\n But your machine only has: []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-c19fb30a9af3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifiers_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainers_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefine_classifier_and_trainer_for_each_cv_fold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_test_splits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokeniser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_sequence_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\College\\NLP\\Assignment_3\\nlp_assignment_3\\Bert_functions.py\u001b[0m in \u001b[0;36mdefine_classifier_and_trainer_for_each_cv_fold\u001b[1;34m(train_test_splits, hyperparameters, other_params, model_name, tokeniser, max_sequence_length)\u001b[0m\n\u001b[0;32m    886\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifiers_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[1;31m# define the trainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m         trainer = pl.Trainer(callbacks=[early_stop_callback, save_top_model_callback],\n\u001b[0m\u001b[0;32m    889\u001b[0m                              \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"max_epochs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m                              \u001b[0mmin_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnr_frozen_epochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\env_vars_connector.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# all args were already moved to kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, logger, checkpoint_callback, callbacks, default_root_dir, gradient_clip_val, process_position, num_nodes, num_processes, gpus, auto_select_gpus, tpu_cores, log_gpu_memory, progress_bar_refresh_rate, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, sync_batchnorm, precision, weights_summary, weights_save_path, num_sanity_val_steps, truncated_bptt_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, terminate_on_nan, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, distributed_backend, automatic_optimization, move_metrics_to_cpu, enable_pl_optimizer, multiple_trainloader_mode, stochastic_weight_avg)\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer_connector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptimizerConnector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         self.accelerator_connector = AcceleratorConnector(\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[0mnum_processes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpu_cores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistributed_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauto_select_gpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msync_batchnorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbenchmark\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[0mreplace_sampler_ddp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamp_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamp_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplugins\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_processes, tpu_cores, distributed_backend, auto_select_gpus, gpus, num_nodes, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, precision, amp_type, amp_level, plugins)\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpick_multiple_gpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallel_device_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_gpu_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_distributed_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\pytorch_lightning\\utilities\\device_parser.py\u001b[0m in \u001b[0;36mparse_gpu_ids\u001b[1;34m(gpus)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mMisconfigurationException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GPUs requested but none are available.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mgpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sanitize_gpu_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\pytorch_lightning\\utilities\\device_parser.py\u001b[0m in \u001b[0;36m_sanitize_gpu_ids\u001b[1;34m(gpus)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mgpu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgpu\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_available_gpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             raise MisconfigurationException(\n\u001b[0m\u001b[0;32m    134\u001b[0m                 \u001b[1;34mf\"You requested GPUs: {gpus}\\n But your machine only has: {all_available_gpus}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             )\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: You requested GPUs: [0]\n But your machine only has: []"
     ]
    }
   ],
   "source": [
    "classifiers_list, trainers_list, modeL_callbacks_list = bf.define_classifier_and_trainer_for_each_cv_fold(train_test_splits, classifier_hyperparams, model_params, predefined_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-quantity",
   "metadata": {},
   "source": [
    "### Fit the defined classifiers to the data\n",
    "\n",
    "Once these classifiers and trainers have been defined, it is time to train the models using them. Each of item in these lists corrosponds to one of the cross-validation folds so We can iterate through these lists to get a few evalutaion metrics for each fold.\n",
    "\n",
    "Due to the way this model is set up, we have a validation accuracy along with a test accuracy score. The validation accuracy score was recorded during training while the test accuracy is obtained through a seperate test after the model is trained. We will record both of these metrics during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the dataframe we output with the evaluation summary statistics\n",
    "fold_eval_df = pd.DataFrame(columns=['fold_no', 'fold_time', 'fold_val_accuracy', 'fold_test_accuracy', 'fold_test_loss'], index=range(len(train_test_splits)))\n",
    "\n",
    "fold_val_accuracies, fold_test_accuracies, fold_test_loss = [], [], []\n",
    "eval_start = time.time()\n",
    "for i, (classifier, trainer, save_top_model_callback) in tqdm(enumerate(zip(classifiers_list, trainers_list, modeL_callbacks_list))):\n",
    "    iteration_start = time.time()\n",
    "    \n",
    "    # fit the model to the data\n",
    "    trainer.fit(classifier, classifier.data)\n",
    "    \n",
    "    # get the time this training took\n",
    "    iteration_duration = time.time() - iteration_start\n",
    "    \n",
    "    # get an accuracy score for the trained model\n",
    "    best_val_accuracy = save_top_model_callback.best_model_score.item()\n",
    "    fold_accuracies.append(best_val_accuracy)\n",
    "\n",
    "    # test the model and gets its test accuracy & test loss\n",
    "    test_results = trainer.test(verbose=False)\n",
    "    print(test_results)\n",
    "    test_accuracy = test_results[0][\"test_acc\"]\n",
    "    test_loss = test_results[0][\"test_loss\"]\n",
    "    fold_test_accuracies.append(test_accuracy)\n",
    "    fold_test_loss.append(test_loss)\n",
    "\n",
    "    # add the results from this fold to the fold evaluation dataframe\n",
    "    fold_eval_df.loc[i, 'fold_no'] = i + 1\n",
    "    fold_eval_df.loc[i, 'fold_time'] = iteration_duration\n",
    "    fold_eval_df.loc[i, 'fold_validation_accuracy'] = best_val_accuracy\n",
    "    fold_eval_df.loc[i, 'fold_test_accuracy'] = test_accuracy\n",
    "    fold_eval_df.loc[i, 'fold_test_loss'] = test_loss\n",
    "\n",
    "    # save the path to the best model\n",
    "    fold_best_model_path.append(save_top_model_callback.best_model_path) \n",
    "    \n",
    "    # collect the garbage & empty the cuda memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-portuguese",
   "metadata": {},
   "source": [
    "### Analyse these models results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-swift",
   "metadata": {},
   "source": [
    "As explained above, during and after training the above metrics, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the fold evaluation scores\n",
    "bf.plot_fold_eval_scores(fold_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the evalutation summary statistics for this model\n",
    "n_test = float(len(fold_test_accuracies))\n",
    "avg_test = sum(fold_test_accuracies) / n_test\n",
    "variance_test = sum([(x-avg_test)**2 for x in fold_test_accuracies]) / n_test\n",
    "n_val = float(len(fold_val_accuracies))\n",
    "avg_val = sum(fold_val_accuracies) / n_val\n",
    "variance_val = sum([(x-avg_val)**2 for x in fold_val_accuracies]) / n_val\n",
    "eval_duration = time.time() - eval_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a datframe with one row summarising the model evaluation\n",
    "eval_values = {'Full Name': model_name,\n",
    "               'Avg Test Accuracy': avg_test,\n",
    "               'Avg Val Accuracy': avg_val,\n",
    "               'Test Accuracy Std Dev': variance_test**0.5,\n",
    "               'Val Accuracy Std Dev': variance_val**0.5,\n",
    "               'Min Test Accuracy': min(fold_test_accuracies),\n",
    "               'Max Test Accuracy': max(fold_test_accuracies),\n",
    "               'Min Val Accuracy': min(fold_val_accuracies),\n",
    "               'Max Val Accuracy': max(fold_val_accuracies),\n",
    "               'Total Time (s)': round(eval_duration, 2),\n",
    "               'All Fold Test Accuracies': str(fold_test_accuracies),\n",
    "               'All Fold Val Accuracies': str(fold_val_accuracies),\n",
    "              }\n",
    "full_eval_df = pd.DataFrame(eval_values, index=[0])\n",
    "full_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_best_model_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-fitness",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27R-BOeB1S_q",
   "metadata": {
    "id": "27R-BOeB1S_q"
   },
   "source": [
    "# Save Best Model outside Logs\n",
    "\n",
    "Rather than manually locating the best model in the lightning logs folder and copying it to another location, use the  library to save a copy. This also gives us the option to save a copy without the training state of the Adam optimiser, reducing model size by about 67%, training parameters and filesystem paths that we may not want to share with users of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-emphasis",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B3wnEclM1S_r",
   "metadata": {
    "id": "B3wnEclM1S_r",
    "outputId": "f3c14329-1a5c-4eb0-db3f-aaf526520768"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "INFO:lightning:GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: None, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "for i, trainer in enumerate(trainer_list):\n",
    "    \n",
    "    # After just having run test(), the best checkpoint is still loaded but that's not a documented feature\n",
    "    # To be on the safe side for future versions we need to \n",
    "    save_best_model(classifier, trainer, fold_num=i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html\n",
    "\n",
    "# after just having run test(), the best checkpoint is still loaded but that's\n",
    "# not a documented feature so to be on the safe side for future versions we\n",
    "# need to explicitly load the best checkpoint:\n",
    "\n",
    "best_model = bf.Classifier.load_from_checkpoint(checkpoint_path = trainer.checkpoint_callback.best_model_path\n",
    "                                             # the hparams including hparams.batch_size appear to have been\n",
    "                                             # saved in the checkpoint automatically\n",
    "                                            )\n",
    "\n",
    "# best_model.save_checkpoint('best.ckpt') does not exist\n",
    "# --> need to wrap model into trainer to be able to save a checkpoint\n",
    "\n",
    "new_trainer = pl.Trainer(resume_from_checkpoint=trainer.checkpoint_callback.best_model_path,\n",
    "                         gpus = -1,  # avoid warnings (-1 = automatic selection)\n",
    "                         # https://github.com/PyTorchLightning/pytorch-lightning/issues/6690\n",
    "                         logger = pl.loggers.TensorBoardLogger(os.path.abspath('lightning_logs')),\n",
    "                        )\n",
    "\n",
    "new_trainer.model = best_model  # @model.setter in plugins/training_type/training_type_plugin.py\n",
    "\n",
    "#new_trainer.save_checkpoint(\"best-model.ckpt\")  # contains absoulte paths and training parameters\n",
    "\n",
    "new_trainer.save_checkpoint(\"best-model-weights-only.ckpt\", True,  # save_weights_only\n",
    "                           )\n",
    "\n",
    "# to just save the bert model in pytorch format and without the classification head, we could follow\n",
    "# https://github.com/PyTorchLightning/pytorch-lightning/issues/3096#issuecomment-686877242\n",
    "best_model.bert.save_pretrained('best-bert-encoder.pt')\n",
    "\n",
    "# Since the lightning module inherits from pytorch, we can save the full network in\n",
    "# pytorch format:\n",
    "torch.save(best_model.state_dict(), 'best-model.pt')\n",
    "\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I0S7QyhE1S_s",
   "metadata": {
    "id": "I0S7QyhE1S_s"
   },
   "source": [
    "Note: The `.ckpt` files are zip files containing a [pickle](https://docs.python.org/3/library/pickle.html) file, version information and various binary files, presumably numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-management",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TttCZoE-1S_s",
   "metadata": {
    "id": "TttCZoE-1S_s"
   },
   "source": [
    "## Load a Model and Test Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2mkPK1Qm1S_s",
   "metadata": {
    "id": "2mkPK1Qm1S_s",
    "outputId": "4aeca248-2a6f-43f7-b5a5-67a675a104ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "INFO:lightning:GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: None, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 20\n",
      "setting tokeniser\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a495e1c9bcb4bef801ed0359fe14944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8949999213218689, 'test_loss': 0.21454036235809326}\n",
      "--------------------------------------------------------------------------------\n",
      "[{'test_loss': 0.21454036235809326, 'test_acc': 0.8949999213218689}]\n"
     ]
    }
   ],
   "source": [
    "best_model = Classifier.load_from_checkpoint(checkpoint_path='best-model-weights-only.ckpt')\n",
    "\n",
    "best_model.eval()  # enter prediction mode, e.g. turn off dropout\n",
    "\n",
    "print(best_model.data.data_split)  # confirm the data is not saved\n",
    "\n",
    "test_dataloader = DataLoader(dataset     = SlicedDocuments(raw_data                    = train_test_splits[xval_run][-1], #test\n",
    "                                                           tokeniser                   = tokeniser,\n",
    "                                                           fraction_for_first_sequence = 0.0,\n",
    "                                                           max_sequence_length         = max_sequence_length,\n",
    "                                                           second_part_as_sequence_B   = False,\n",
    "                                                           preproc_batch_size          = 8\n",
    "                                                          ),\n",
    "                             batch_size  = best_model.hparams.batch_size,\n",
    "                             collate_fn  = best_model.prepare_sample,\n",
    "                             num_workers = best_model.hparams.loader_workers,\n",
    "                            )\n",
    "\n",
    "print('number of batches:', len(test_dataloader))\n",
    "\n",
    "new_trainer = pl.Trainer(gpus = -1,\n",
    "                         # https://github.com/PyTorchLightning/pytorch-lightning/issues/6690\n",
    "                         logger = pl.loggers.TensorBoardLogger(os.path.abspath('lightning_logs')),\n",
    "                        )\n",
    "\n",
    "if best_model.tokenizer is None:\n",
    "    print('setting tokeniser')\n",
    "    best_model.tokenizer = tokeniser\n",
    "\n",
    "print(new_trainer.test(best_model, test_dataloaders=[test_dataloader]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-diploma",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sZAf76Xm1S_s",
   "metadata": {
    "id": "sZAf76Xm1S_s"
   },
   "source": [
    "## Make Predictions\n",
    "\n",
    "Pytorch_lightning does not seem to provide functionality to re-use above code for making predictions. The example code from their website directly calls the `forward()` function of the model, assuming that the inputs of the test items are ready in a suitable batch. For a small test set that does not exceed the batch size, we can manally create such as a batch as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BywMwdo81S_t",
   "metadata": {
    "id": "BywMwdo81S_t"
   },
   "source": [
    "### Small Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KgxLqKSN1S_t",
   "metadata": {
    "id": "KgxLqKSN1S_t",
    "outputId": "7ab1929f-9279-45bb-d0b7-8c46f2459f52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model device: cuda:0\n",
      "number of documents: 2\n"
     ]
    }
   ],
   "source": [
    "# reminder: In our dataset, documents are lists of sentences\n",
    "# and each sentence is a list of words and punctuation\n",
    "\n",
    "mini_test_set  = [# document 1\n",
    "                  ([['This', 'movie', 'is', 'great', '.'], ['So', 'much', 'fun', '.']], 'pos'),\n",
    "                  # document 2\n",
    "                  ([['What', 'a', 'waste', 'of', 'time', '.'], ['Never', 'seen', 'anything', 'this', 'bad', '.']], 'neg'),\n",
    "                 ]\n",
    "\n",
    "dataset = SlicedDocuments(# subclass of torch.utils.data.Dataset\n",
    "                          mini_test_set,\n",
    "                          preproc_batch_size = 8,\n",
    "                          # the following should match the trained model\n",
    "                          tokeniser = tokeniser,\n",
    "                          fraction_for_first_sequence = 0.0,  \n",
    "                          max_sequence_length = max_sequence_length,\n",
    "                          second_part_as_sequence_B = False,\n",
    "                         )\n",
    "\n",
    "print('model device:', best_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sxSg07yx1S_t",
   "metadata": {
    "id": "sxSg07yx1S_t",
    "outputId": "7b973236-6eae-4192-d1ae-1911f98690f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items in batch: 4\n",
      "number of predictions: 2\n",
      "[0] {'parts': [['This', 'movie', 'is', 'great', '.', 'So', 'much', 'fun', '.']], 'label': 'pos'}\n",
      "prediction: pos\n",
      "[1] {'parts': [['What', 'a', 'waste', 'of', 'time', '.', 'Never', 'seen', 'anything', 'this', 'bad', '.']], 'label': 'neg'}\n",
      "prediction: neg\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "encoded_batch, gold_labels = best_model.prepare_sample(dataset)\n",
    "print('number of items in batch:', len(encoded_batch))  # TODO: Why is this not len(dataset)?\n",
    "\n",
    "best_model.eval()  # just in case (already called further above)\n",
    "\n",
    "best_model.freeze()  # some examples call this before making predictions\n",
    "\n",
    "# https://github.com/huggingface/transformers/issues/5111\n",
    "encoded_batch.to(best_model.device)\n",
    "\n",
    "model_out = best_model(encoded_batch)\n",
    "\n",
    "# adjsuted copy of code from predict()\n",
    "logits = model_out[\"logits\"]\n",
    "logits = torch.Tensor.cpu(logits).numpy()\n",
    "predicted_labels = [best_model.data.label_encoder.index_to_token[prediction] for prediction in numpy.argmax(logits, axis=1)]\n",
    "\n",
    "print('number of predictions:', len(predicted_labels))  # matches len(dataset)\n",
    "\n",
    "for index, item in enumerate(dataset):\n",
    "    print('[%d]' %index, item)\n",
    "    print('prediction:', predicted_labels[index])\n",
    "    \n",
    "# the 'parts' list has two parts when second_part_as_sequence_B = True and fraction_for_first_sequence > 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "np85gTr-1S_u",
   "metadata": {
    "id": "np85gTr-1S_u"
   },
   "source": [
    "### Large Test Sets\n",
    "For test sets that do not fit into a single batch, we extend the model's evaluation function to also record predictions in the metrics dictionary. We keep a record of the inputs as well as the test items may be distributed over multiple GPUs and the order of items may therefore change. We then only need to tokenise the test items again and fetch the predictions from the metrics dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "IVMWsARj1S_u",
   "metadata": {
    "id": "IVMWsARj1S_u",
    "outputId": "512a7b9d-25cc-400e-d301-1863fe2d224c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-2accf16b206b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# uses best_model, dataset and new_trainer from above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_recording_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m new_trainer.test(best_model,\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "# uses best_model, dataset and new_trainer from above\n",
    "\n",
    "best_model.start_recording_predictions()\n",
    "\n",
    "new_trainer.test(best_model,\n",
    "                 test_dataloaders=[DataLoader(dataset     = dataset, # First test the functionality with a small test set\n",
    "                                              batch_size  = best_model.hparams.batch_size,\n",
    "                                              collate_fn  = best_model.prepare_sample,\n",
    "                                              num_workers = best_model.hparams.loader_workers,\n",
    "                                             )\n",
    "                                  ]\n",
    "                )\n",
    "\n",
    "best_model.stop_recording_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eJj3xii91S_u",
   "metadata": {
    "id": "eJj3xii91S_u",
    "outputId": "1e534602-6dac-4178-e320-4d0efe9f54d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(101, 2023, 3185, 2003, 2307, 1012, 2061, 2172, 4569, 1012, 102): 'pos', (101, 2054, 1037, 5949, 1997, 2051, 1012, 2196, 2464, 2505, 2023, 2919, 1012, 102): 'neg'}\n"
     ]
    }
   ],
   "source": [
    "print(best_model.seq2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xO4A6zmh1S_u",
   "metadata": {
    "id": "xO4A6zmh1S_u",
    "outputId": "1cfb2661-74b5-4a93-8f7d-bfc415281a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] {'parts': [['This', 'movie', 'is', 'great', '.', 'So', 'much', 'fun', '.']], 'label': 'pos'}\n",
      "prediction: pos\n",
      "[1] {'parts': [['What', 'a', 'waste', 'of', 'time', '.', 'Never', 'seen', 'anything', 'this', 'bad', '.']], 'label': 'neg'}\n",
      "prediction: neg\n"
     ]
    }
   ],
   "source": [
    "for index, item in enumerate(dataset):\n",
    "    print('[%d]' %index, item)\n",
    "    input_token_ids = best_model.prepare_sample([item])[0]['input_ids']\n",
    "    key = input_token_ids.tolist()[0]\n",
    "    \n",
    "    # truncate zeros\n",
    "    while key and key[-1] == 0:\n",
    "        del key[-1]\n",
    "\n",
    "    key = tuple(key)\n",
    "\n",
    "    try:\n",
    "        print('prediction:', best_model.seq2label[key])\n",
    "    except KeyError:\n",
    "        print('prediction not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-12TiO6p1S_v",
   "metadata": {
    "id": "-12TiO6p1S_v",
    "outputId": "aa8bd403-2af1-40a2-b36d-f3a9b79e08bb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc346e3cfb24c528a919b9122c3e4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8949999213218689, 'test_loss': 0.21454036235809326}\n",
      "--------------------------------------------------------------------------------\n",
      "Ready\n"
     ]
    }
   ],
   "source": [
    "# now with a bigger dataset\n",
    "\n",
    "best_model.start_recording_predictions()\n",
    "\n",
    "xval_test_dataset = SlicedDocuments(raw_data                    = train_test_splits[xval_run][-1],  # test data\n",
    "                                    tokeniser                   = tokeniser,\n",
    "                                    fraction_for_first_sequence = 0.0,\n",
    "                                    max_sequence_length         = max_sequence_length,\n",
    "                                    second_part_as_sequence_B   = False,\n",
    "                                    preproc_batch_size          = 8\n",
    "                                   )\n",
    "\n",
    "new_trainer.test(best_model,\n",
    "                 test_dataloaders=[DataLoader(dataset     = xval_test_dataset,    \n",
    "                                              batch_size  = best_model.hparams.batch_size,\n",
    "                                              collate_fn  = best_model.prepare_sample,\n",
    "                                              num_workers = best_model.hparams.loader_workers,\n",
    "                                             )\n",
    "                                  ]\n",
    "                )\n",
    "\n",
    "best_model.stop_recording_predictions()\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = bf.test_model_and_get_results(best_model, xval_test_dataset)\n",
    "prediction_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sentiment-bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
